{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import time\n",
    "import copy\n",
    "\n",
    "import resnet as rnet\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda_available else \"cpu\")\n",
    "print(f'''using device {device}''')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ola/Documents/DD2424project\n"
     ]
    }
   ],
   "source": [
    "path = !pwd\n",
    "path = path[0]\n",
    "print(path)\n",
    "mdl_savefile = path+\"/models/resnet56_cumultrain_parameters.pickle\"\n",
    "mdl_loadfile = mdl_savefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model(model):\n",
    "    print(\"Model's state_dict:\")\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "def make_oneh(labels):\n",
    "    oneh_labels = np.zeros([labels.shape[0], 10])\n",
    "    indexes = np.array([np.arange(0,labels.shape[0]), labels.numpy()])\n",
    "    oneh_labels[tuple(indexes)] = 1.0\n",
    "    return torch.tensor(oneh_labels).long()\n",
    "\n",
    "def get_xent_acc(prediction, labels):\n",
    "    predicted_labels = np.argmax(prediction.cpu().data.numpy(),1)\n",
    "    (predicted_labels - labels.cpu().data.numpy())\n",
    "    return sum((predicted_labels - labels.cpu().data.numpy()) == 0)/predicted_labels.shape[0]\n",
    "\n",
    "def xent_softmax(prediction):\n",
    "    pred = np.exp(prediction.cpu().data.numpy())\n",
    "    divide = np.repeat(pred.sum(1).reshape(pred.shape[0],1), pred.shape[1],1)\n",
    "    pred = np.divide(pred,divide)\n",
    "    return pred \n",
    "\n",
    "def update_results(n_vbatches=1):\n",
    "    f\"updates loss and accuracy for training and validation\" \n",
    "    global mom_track\n",
    "    global lr_track\n",
    "    global accu_tr\n",
    "    global accu_val\n",
    "    global loss_tr\n",
    "    global loss_val\n",
    "    mom_track += [optimizer.param_groups[-1]['momentum']]\n",
    "    lr_track  += [optimizer.param_groups[-1]['lr']]\n",
    "    accu_tr   += [get_xent_acc(prediction, labels)]\n",
    "    loss_tr   += [output.data.cpu().numpy().item()]\n",
    "    with torch.no_grad():\n",
    "        network.eval()      \n",
    "        accu, loss = 0, 0\n",
    "        for i, (imgs_val, labels_val) in enumerate(dataloader_vl):\n",
    "            imgs_val, labels_val = cuda(imgs_val), cuda(labels_val)\n",
    "            prediction_val = network.forward(imgs_val)  \n",
    "            accu += get_xent_acc(prediction_val, labels_val)\n",
    "            loss +=       lossfn(prediction_val, labels_val).data.cpu().numpy().item()\n",
    "            if i == n_vbatches:\n",
    "                break\n",
    "        loss_val = loss_val + [loss/(i+1)]\n",
    "        accu_val = accu_val + [accu/(i+1)]\n",
    "        network.train()\n",
    "\n",
    "def compare_parameters(net1, net2):\n",
    "    f\"\"\"compare parameters of two models with the same architecture\"\"\" \n",
    "    net1_param, net2_param, diff = [], [], 0\n",
    "    for param_tensor in net1.state_dict():\n",
    "        net1_param = net1_param + [net1.state_dict()[param_tensor]]\n",
    "        net2_param = net2_param + [net2.state_dict()[param_tensor]]\n",
    "    for i in range(len(net1_param)):\n",
    "        diff = diff + np.sum(net1_param[i].cpu().numpy() - net2_param[i].cpu().numpy())\n",
    "    print(f\"\"\"difference in weights and biases is: {diff} between network1 and network2\"\"\")\n",
    "\n",
    "def load_cifar10(bsize_tr,bsize_val=128):\n",
    "    torchvision.datasets.CIFAR10(path,download=True)\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])\n",
    "    cifar10_train = torchvision.datasets.CIFAR10(path, train=True, transform=transform)\n",
    "    cifar10_valid = torchvision.datasets.CIFAR10(path, train=False,transform=transform)\n",
    "    dataloader_tr = torch.utils.data.DataLoader(cifar10_train,\n",
    "                                              batch_size=bsize_tr,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=8,\n",
    "                                              pin_memory=True)\n",
    "    dataloader_val = torch.utils.data.DataLoader(cifar10_valid,\n",
    "                                              batch_size=bsize_val,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=8,\n",
    "                                              pin_memory=True)\n",
    "    return dataloader_tr, dataloader_val\n",
    "\n",
    "def cuda(thing):\n",
    "    if torch.cuda.is_available(): return thing.cuda()        \n",
    "    return thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = rnet.resnet56()\n",
    "for layer in network.layer1:\n",
    "    print(layer) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.layer1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning-rate range-test\n",
    "\n",
    "(1) a factor of 3 or 4 less than the maximum bound, (2) a factor of 10 or 20 less than the maximum bound if only one cycle is used, (3) by a short test of hundreds of iterations with a few initial learning rates and pick the largest one that allows convergence to begin without signs of overfitting as shown in Figure 1a (if the initial learning rate is too large, the training won’t begin to converge). Take note that there is a maximum speed the learning rate can increase without the training becoming unstable, which effects your choices for the minimum and maximum learning rates (i.e., increase the stepsize to increase the difference between the minimum and maximum).\n",
    "\n",
    "Learning rate (LR): Perform a learning rate range test to a “large” learning rate. The max LR depends on the architecture (for the shallow 3-layer architecture, large is 0.01 while for resnet, large is 3.0), you might try more than one maximum. Using the 1cycle LR policy with a maximum learning rate determined from an LR range test, a minimum learning rate as a tenth of the maximum appears to work well but other factors are relevant, such as the rate of learning rate increase (too fast and increase will cause instabilities).\n",
    "\n",
    "\n",
    "SuperConvergence paper recommends minimum learning-rate to be 1/3rd or 1/4th of the maximum learning-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lr      = 1e-9\n",
    "max_lr      = 1.5e1\n",
    "iterations  = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfn      = nn.CrossEntropyLoss()\n",
    "dataloader_tr, dataloader_vl = load_cifar10(bsize_tr)\n",
    "start      = time.time()    \n",
    "n_vbatches = 10\n",
    "intvl_val  = 10\n",
    "intvl_time = 100 \n",
    "results_lrate = [] \n",
    "\n",
    "for bsize_tr in [512]:\n",
    "    accu_tr, accu_val, loss_tr, loss_val = [], [], [], []    \n",
    "    network   = rnet.resnet56()\n",
    "    if cuda_available: network.cuda()\n",
    "    optimizer = optim.SGD(network.parameters(), lr=min_lr)    \n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                                  min_lr, \n",
    "                                                  max_lr, \n",
    "                                                  step_size_up=iterations, \n",
    "                                                  step_size_down=0, \n",
    "                                                  mode='triangular', \n",
    "                                                  cycle_momentum=False, \n",
    "                                                  base_momentum=0)\n",
    "    i = 1\n",
    "    while i < iterations:\n",
    "        datait_tr = iter(dataloader_tr)\n",
    "        for n, (images, labels) in enumerate(datait_tr):\n",
    "            images, labels = cuda(images), cuda(labels)\n",
    "            prediction     = network.forward(images)\n",
    "            output         = lossfn(prediction, labels)\n",
    "            if i % intvl_val == 0: update_results(n_vbatches=n_vbatches)\n",
    "            optimizer.zero_grad()  # clear old gradients\n",
    "            output.backward()      # calculate new gradients \n",
    "            optimizer.step()       # updates weights\n",
    "            scheduler.step()       # change learning-rate in optimizer\n",
    "            if i % intvl_time == 0: print(f\"time after {i} iterations: {time.time() - start} seconds\")                \n",
    "            i += 1\n",
    "            if i >= iterations: break\n",
    "    f\"deepcopy results and make room for another run\"\n",
    "    results_lrate.append({'max_lr'  :max_lr,\n",
    "                          'bsize_tr':bsize_tr,\n",
    "                          'accu_tr' :copy.deepcopy(np.array(accu_tr)), \n",
    "                          'accu_val':copy.deepcopy(np.array(accu_val)), \n",
    "                          'loss_tr' :copy.deepcopy(np.array(loss_tr)), \n",
    "                          'loss_val':copy.deepcopy(np.array(loss_val))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment += 1\n",
    "x1, x2 = [np.linspace(min_lr, max_lr, num) for num in [len(accu_val), len(loss_val)]]\n",
    "plt.plot(x1, np.array(accu_val))\n",
    "save_file   = path+f\"/rangetest/ola/{experiment}LRRT_bs{bsize_tr}minlr{min_lr}maxlr{max_lr}iter{iterations}mom0wd0{arch}.png\"\n",
    "plt.savefig(save_file)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = [np.linspace(max_lr, min_lr, num) for num in [len(accuracy), len(loss)]]\n",
    "plt.plot(x1, np.array(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate best batchsize \n",
    "hyperparameters 1 recommends to use as large batchsize as possible, batchsize is choosen at the same time as choosing learningrate range\n",
    "\n",
    "Total batch size (TBS): A large batch size works well but the magnitude is typically con- strained by the GPU memory. If your server has multiple GPUs, the total batch size is the batch size on a GPU multiplied by the number of GPUs. If the architecture is small or your hardware permits very large batch sizes, then you might compare performance of different batch sizes. In addition, recall that small batch sizes add regularization while large batch sizes add less, so utilize this while balancing the proper amount of regularization. It is often better to use a larger batch size so a larger learning rate can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate best momentum value\n",
    "\n",
    "Momentum: Short runs with momentum values of 0.99, 0.97, 0.95, and 0.9 will quickly show the best value for momentum. If using the 1cycle learning rate schedule, it is better to use a cyclical momentum (CM) that starts at this maximum momentum value and decreases with increasing learning rate to a value of 0.8 or 0.85 (performance is almost independent of the minimum momentum value). Using cyclical momentum along with the LR range test stabilizes the convergence when using large learning rate values more than a constant momentum does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max   = 1.0\n",
    "lr_min   = lr_max/10.0\n",
    "bsize_tr = 512\n",
    "step_it  = 150\n",
    "mom_min  = 0.8\n",
    "n_cycles   = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dataloader_tr, dataloader_vl = load_cifar10(bsize_tr) \n",
    "lossfn      = nn.CrossEntropyLoss()\n",
    "results_momentum = []\n",
    "intvl_val  = 10\n",
    "intvl_time = 100\n",
    "momentums_max = [0.9, 0.95, 0.97, 0.99]\n",
    "for mom_max in momentums_max:    \n",
    "    accu_tr, accu_val, loss_tr, loss_val = [], [], [], []\n",
    "    network   = rnet.resnet56()\n",
    "    if cuda_available: network.cuda()\n",
    "    optimizer = optim.SGD(network.parameters(), lr=lr_min)\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              lr_min, \n",
    "                                              lr_max, \n",
    "                                              step_size_up=step_it, \n",
    "                                              step_size_down=step_it, \n",
    "                                              mode='triangular', \n",
    "                                              cycle_momentum=True, \n",
    "                                              base_momentum=mom_min,\n",
    "                                              max_momentum=mom_max)\n",
    "    f\"do short run\"\n",
    "    for cycle in range(1,n_cycles+1):\n",
    "        i = 1\n",
    "        while i <= step_it*2:\n",
    "            datait_tr = iter(dataloader_tr)\n",
    "            for n, (images, labels) in enumerate(datait_tr):\n",
    "                images, labels = cuda(images), cuda(labels)\n",
    "                prediction     = network.forward(images)\n",
    "                output         = lossfn(prediction, labels)\n",
    "                if i % intvl_val  == 0: update_results(n_vbatches=10)                 \n",
    "                optimizer.zero_grad()  # clear old gradients\n",
    "                output.backward()      # calculate new gradients \n",
    "                optimizer.step()       # updates weights\n",
    "                scheduler.step()       # change learning-rate in optimizer\n",
    "                if i % intvl_time == 0: print(f\"time after cycle {cycle} and {i} iterations: {time.time() - start} seconds\")                \n",
    "                i += 1                    \n",
    "                if i > step_it*2: break\n",
    "\n",
    "    f\"deepcopy results and make room for another run\"\n",
    "    results_momentum.append({'mom_max' :mom_max,\n",
    "                             'accu_tr' :copy.deepcopy(np.array(accu_tr)), \n",
    "                             'accu_val':copy.deepcopy(np.array(accu_val)), \n",
    "                             'loss_tr' :copy.deepcopy(np.array(loss_tr)), \n",
    "                             'loss_val':copy.deepcopy(np.array(loss_val))})\n",
    "print(f\"complete\\n the whole process took: {(time.time() - start)/60.0} minutes\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,17))\n",
    "for i, result in enumerate(results_momentum):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    array_toplot = result['accu_val']\n",
    "    legend_string = f\"max momentum {result['mom_max']}\"\n",
    "    x1 = np.linspace(1,step_it*2,len(array_toplot))\n",
    "    plt.plot(x1,array_toplot)\n",
    "    plt.legend(handles=[mpatches.Patch(label=legend_string)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"analyse momentum results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate best weight-decay\n",
    "\n",
    "Weight decay (WD): This requires a grid search to determine the proper magnitude but usually does not require more than one significant figure accuracy. Use your knowl- edge of the dataset and architecture to decide which values to test. For example, a more complex dataset requires less regularization so test smaller weight decay values, such as 10−4, 10−5, 10−6, 0. A shallow architecture requires more regularization so test larger weight decay values, such as 10−2, 10−3, 10−4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max   = 1.0\n",
    "lr_min   = lr_max/10.0\n",
    "bsize_tr = 512\n",
    "mom_max  = 0.9\n",
    "mom_min  = 0.8\n",
    "step_epo = 5\n",
    "step_it  = 50000/bsize_tr*step_epo\n",
    "anne_epo = 1 \n",
    "anne_it  = 50000/bsize_tr*anne_epo\n",
    "cycles   = 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in hyperparameter1 the number of steps up and down are described in epochs – which depends on the batchsize\n",
    "cifar10 has 50.000 images. each epoch has 50000/bsize_tr steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dataloader_tr, dataloader_vl = load_cifar10(bsize_tr)\n",
    "lossfn      = nn.CrossEntropyLoss()\n",
    "results_weightdecay = [] \n",
    "intvl_val  = 10\n",
    "intvl_time = 100\n",
    "for j, wd in enumerate([1e-4, 1e-5, 1e-6]):\n",
    "    accu_tr, accu_val, loss_tr, loss_val = [], [], [], []\n",
    "    network   = rnet.resnet56()\n",
    "    if cuda_available: network.cuda()\n",
    "    optimizer = optim.SGD(network.parameters(), lr=lr_min)\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                                  lr_min, \n",
    "                                                  lr_max, \n",
    "                                                  step_size_up=step_it, \n",
    "                                                  step_size_down=step_it, \n",
    "                                                  mode='triangular', \n",
    "                                                  cycle_momentum=True, \n",
    "                                                  base_momentum=mom_min,\n",
    "                                                  max_momentum=mom_max)\n",
    "    f\"do Cyclic Learning-Rate training\"\n",
    "    for cycle in range(1,cycles+1):\n",
    "        i = 1\n",
    "        while i < step_it*2:\n",
    "            datait_tr = iter(dataloader_tr)\n",
    "            for n, (images, labels) in enumerate(datait_tr):\n",
    "                images, labels = cuda(images), cuda(labels)\n",
    "                prediction     = network.forward(images)\n",
    "                output         = lossfn(prediction, labels)\n",
    "                if i % intvl_val  == 0: update_results(n_vbatches=10)                 \n",
    "                optimizer.zero_grad()  # clear old gradients\n",
    "                output.backward()      # calculate new gradients \n",
    "                optimizer.step()       # updates weights\n",
    "                scheduler.step()       # change learning-rate in optimizer\n",
    "                if i % intvl_time == 0: print(f\"time after {i} iterations: {time.time() - start} seconds\")                \n",
    "                i += 1                    \n",
    "                if i >= step_it*2 :break\n",
    "    f\"deepcopy results and make room for another run\"\n",
    "    results_weightdecay.append({'wd'      :wd,\n",
    "                                'accu_tr' :copy.deepcopy(np.array(accu_tr)), \n",
    "                                'accu_val':copy.deepcopy(np.array(accu_val)), \n",
    "                                'loss_tr' :copy.deepcopy(np.array(loss_tr)), \n",
    "                                'loss_val':copy.deepcopy(np.array(loss_val))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(17,17))\n",
    "for i, result in enumerate(results_weightdecay):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    array_toplot = result['accu_val']\n",
    "    legend_string = f\"weight-decay {result['wd']}\"\n",
    "    x1 = np.linspace(1,step_it*2,len(array_toplot))\n",
    "    plt.plot(x1,array_toplot)\n",
    "    plt.legend(handles=[mpatches.Patch(label=legend_string)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in enumerate(results_weightdecay):\n",
    "    print(result['accu_val'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"analyse weightdecay results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SuperConvergence Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max   = 3.0\n",
    "lr_min   = lr_max/10.0\n",
    "bsize_tr = 512\n",
    "mom_max  = 0.9\n",
    "mom_min  = 0.8\n",
    "step_epo = 10\n",
    "step_it  = 50000/bsize_tr*step_epo\n",
    "anne_epo = 2\n",
    "anne_it  = 50000/bsize_tr*anne_epo\n",
    "cycles   = 1\n",
    "wd       = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dataloader_tr, dataloader_vl = load_cifar10(bsize_tr)\n",
    "lossfn      = nn.CrossEntropyLoss()\n",
    "results_superconvergence = [] \n",
    "intvl_val  = 10\n",
    "intvl_time = 100\n",
    "\n",
    "accu_tr, accu_val, loss_tr, loss_val, lr_track, mom_track = [], [], [], [], [], []\n",
    "network   = rnet.resnet56()\n",
    "if cuda_available: network.cuda()\n",
    "optimizer = optim.SGD(network.parameters(), lr=lr_min)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              lr_min, \n",
    "                                              lr_max, \n",
    "                                              step_size_up=step_it, \n",
    "                                              step_size_down=step_it, \n",
    "                                              mode='triangular', \n",
    "                                              cycle_momentum=True, \n",
    "                                              base_momentum=mom_min,\n",
    "                                              max_momentum=mom_max)\n",
    "f\"do Cyclic Learning-Rate training\"\n",
    "for cycle in range(1,cycles+1):\n",
    "    i = 1\n",
    "    while i < step_it*2:\n",
    "        datait_tr = iter(dataloader_tr)\n",
    "        for n, (images, labels) in enumerate(datait_tr):\n",
    "            images, labels = cuda(images), cuda(labels)\n",
    "            prediction     = network.forward(images)\n",
    "            output         = lossfn(prediction, labels)\n",
    "            if i % intvl_val  == 0: update_results(n_vbatches=10)                 \n",
    "            optimizer.zero_grad()  # clear old gradients\n",
    "            output.backward()      # calculate new gradients \n",
    "            optimizer.step()       # updates weights\n",
    "            scheduler.step()       # change learning-rate in optimizer\n",
    "            if i % intvl_time == 0: print(f\"time after {i} iterations: {time.time() - start} seconds\")                \n",
    "            i += 1                    \n",
    "            if i >= step_it*2 :break\n",
    "f\"deepcopy results and make room for another run\"\n",
    "results_superconvergence.append({'mom_track':copy.deepcopy(np.array(mom_track)),  \n",
    "                                 'lr_track':copy.deepcopy(np.array(lr_track)),\n",
    "                                 'accu_tr' :copy.deepcopy(np.array(accu_tr)), \n",
    "                                 'accu_val':copy.deepcopy(np.array(accu_val)), \n",
    "                                 'loss_tr' :copy.deepcopy(np.array(loss_tr)), \n",
    "                                 'loss_val':copy.deepcopy(np.array(loss_val))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model before annealing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,17))\n",
    "for i, result in enumerate(results_superconvergence):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    array_toplot = result['accu_val']\n",
    "    legend_string = f\"superconvergence\"\n",
    "    x1 = np.linspace(1,step_it*2*cycles,len(array_toplot))\n",
    "    plt.plot(x1,array_toplot)\n",
    "    plt.legend(handles=[mpatches.Patch(label=legend_string)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 2\n",
    "save_model_filename  = path+f'/models/{experiment}maf0.95resnet56supconv_beforeanne_lrmax{lr_max}lrmin{lr_min}bs{bsize_tr}mommax{mom_max}mommin{mom_min}wd{wd}it{step_it}anit{anne_it}'\n",
    "save_result_filename = path+f'/models/{experiment}maf0.95resnet56supconv_beforeanne_result'\n",
    "save_opt_filename    = path+f'/models/{experiment}opt_state_dict'\n",
    "load_model_filename  = save_model_filename\n",
    "load_result_filename = save_result_filename\n",
    "load_opt_filename    = save_opt_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(network.state_dict(), save_model_filename)\n",
    "\n",
    "# with open(save_result_filename, 'wb') as handle:\n",
    "#     pickle.dump(results_superconvergence, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# opt_state_dict = optimizer.state_dict()\n",
    "# with open(save_opt_filename, 'wb') as handle:\n",
    "#     pickle.dump(opt_state_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and do Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ola/Documents/DD2424project/resnet.py:37: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(m.weight)\n"
     ]
    }
   ],
   "source": [
    "network = rnet.resnet56()\n",
    "network = cuda(network)\n",
    "network.load_state_dict(torch.load(load_model_filename))\n",
    "\n",
    "with open(load_result_filename, 'rb') as handle:\n",
    "    results_superconvergence = pickle.load(handle)\n",
    "\n",
    "optimizer = optim.SGD(network.parameters(), lr=lr_min)\n",
    "with open(load_opt_filename, 'rb') as handle:\n",
    "    loaded_opt_state_dict = pickle.load(handle)\n",
    "optimizer.load_state_dict(loaded_opt_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(network.parameters(), lr=lr_min)\n",
    "scheduler_anne = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              lr_min/10.0, \n",
    "                                              lr_min, \n",
    "                                              step_size_up=1, \n",
    "                                              step_size_down=anne_it*2, \n",
    "                                              mode='triangular', \n",
    "                                              cycle_momentum=True, \n",
    "                                              base_momentum=mom_max,\n",
    "                                              max_momentum=mom_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2937792\n",
      "0.293088\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.param_groups[-1]['lr'])\n",
    "scheduler_anne.step()\n",
    "print(optimizer.param_groups[-1]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "time after 100 iterations: 25.307459831237793 seconds\n",
      "time after 200 iterations: 48.775346755981445 seconds\n",
      "time after 300 iterations: 72.08799576759338 seconds\n"
     ]
    }
   ],
   "source": [
    "f\"do annealing\"\n",
    "start = time.time()\n",
    "lr_track, mom_track = list(copy.deepcopy(results_superconvergence[0]['lr_track'])), list(copy.deepcopy(results_superconvergence[0]['mom_track']))\n",
    "accu_tr, accu_val   = list(copy.deepcopy(results_superconvergence[0]['accu_tr'])), list(copy.deepcopy(results_superconvergence[0]['accu_val']))\n",
    "loss_tr, loss_val   = list(copy.deepcopy(results_superconvergence[0]['loss_tr'])), list(copy.deepcopy(results_superconvergence[0]['loss_val']))\n",
    "results_annealing   = []\n",
    "optimizer = optim.SGD(network.parameters(), lr=lr_min)\n",
    "dataloader_tr, dataloader_vl = load_cifar10(bsize_tr)\n",
    "lossfn      = nn.CrossEntropyLoss()\n",
    "intvl_val  = 10\n",
    "intvl_time = 100\n",
    "scheduler_anne = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              lr_min/10.0, \n",
    "                                              lr_min, \n",
    "                                              step_size_up=1, \n",
    "                                              step_size_down=anne_it*2, \n",
    "                                              mode='triangular', \n",
    "                                              cycle_momentum=True, \n",
    "                                              base_momentum=mom_max,\n",
    "                                              max_momentum=mom_max)\n",
    "i = 1\n",
    "while i < anne_it*2:\n",
    "    datait_tr = iter(dataloader_tr)\n",
    "    for n, (images, labels) in enumerate(datait_tr):\n",
    "        images, labels = cuda(images), cuda(labels)\n",
    "        prediction     = network.forward(images)\n",
    "        output         = lossfn(prediction, labels)\n",
    "        if i % intvl_val  == 0: update_results(n_vbatches=10)                 \n",
    "        optimizer.zero_grad()  # clear old gradients\n",
    "        output.backward()      # calculate new gradients \n",
    "        optimizer.step()       # updates weights\n",
    "        scheduler_anne.step()  # change learning-rate in optimizer\n",
    "        if i % intvl_time == 0: print(f\"time after {i} iterations: {time.time() - start} seconds\")                \n",
    "        i += 1                    \n",
    "        if i >= anne_it*2 :break\n",
    "f\"deepcopy results and make room for another run\"\n",
    "results_annealing.append({'mom_track':copy.deepcopy(np.array(mom_track)),  \n",
    "                          'lr_track' :copy.deepcopy(np.array(lr_track)),  \n",
    "                          'accu_tr'  :copy.deepcopy(np.array(accu_tr)), \n",
    "                          'accu_val' :copy.deepcopy(np.array(accu_val)), \n",
    "                          'loss_tr'  :copy.deepcopy(np.array(loss_tr)), \n",
    "                          'loss_val' :copy.deepcopy(np.array(loss_val))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7734375"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['accu_val'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x1224 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(17,17))\n",
    "for i, result in enumerate(results_annealing):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    array_toplot = result['loss_val']\n",
    "    legend_string = f\"annealing\"\n",
    "    x1 = np.linspace(1,step_it*2+anne_it*2,len(array_toplot))\n",
    "    x2 = np.linspace(1,step_it*2+anne_it*2,len(result['loss_tr']))\n",
    "    plt.plot(x1,array_toplot)\n",
    "    plt.plot(x2,result['loss_tr'])\n",
    "    plt.legend(handles=[mpatches.Patch(label=legend_string)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_toplot[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now do proper training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 128\n",
    "dataloader_tr, dataloader_vl = load_cifar10(batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "f\"\"\"To get ResNet56 use ResNet(BasicBlock,[9,9,9])\"\"\" \n",
    "resnet56 = ResNet(BasicBlock,[9,9,9])\n",
    "if cuda_available: resnet56.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss-function    https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_xent = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimize-function https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(resnet56.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# play with forward and backwards pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start    = time.time()\n",
    "f\"\"\"load previous model\"\"\"\n",
    "# if os.path.isfile(load_filename):\n",
    "#     resnet.load_state_dict(torch.load(load_filename))\n",
    "n_epoch  = 1\n",
    "validation_interval = 10\n",
    "\n",
    "trainacc  = [] \n",
    "valacc    = [] \n",
    "trainloss = []\n",
    "valloss   = [] \n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    train_iterator = iter(train_data_loader)\n",
    "    for i in range(len(train_iterator)):\n",
    "        images, labels = train_iterator.next()\n",
    "        \n",
    "        f\"\"\"if we use GPU, input- and target-tensors must be loaded on GPU as well.\"\"\"\n",
    "        if cuda_available: images, labels = images.cuda(), labels.cuda()\n",
    "            \n",
    "        prediction = resnet.forward(images)\n",
    "        output     = loss_xent(prediction, labels)\n",
    "        \n",
    "        trainacc = trainacc + [get_xent_acc(prediction, labels)]\n",
    "        trainloss = trainloss + [output]\n",
    "        \n",
    "        f\"\"\"using zero_grad() seems to be necessary. step() does not clear the gradients\"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        output.backward() # calculates gradients \n",
    "        optimizer.step()  # updates weights\n",
    "        \n",
    "        if i % validation_interval == 0:\n",
    "            valacc, valloss = get_valacc(valacc, valloss)\n",
    "            \n",
    "    print(f\"\"\"time passed after training {epoch+1} epochs is {time.time()-start} seconds\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, x3 = [np.linspace(0, len(trainacc), num) for num in [len(trainloss), len(trainacc), len(valacc)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x1,np.array(trainloss))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x2,np.array(trainacc))\n",
    "plt.plot(x3,np.array(valacc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load Model\n",
    "### https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "f\"\"\"save\"\"\"\n",
    "torch.save(resnet.state_dict(), save_filename)\n",
    "f\"\"\"load\"\"\"\n",
    "resnet2 = ResNet(BasicBlock,[9,9,9])\n",
    "resnet2.load_state_dict(torch.load(load_filename))\n",
    "compare_parameters(resnet,resnet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(torchvision.utils.make_grid(images.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet = models.densenet161()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
