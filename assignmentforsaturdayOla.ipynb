{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import time\n",
    "import copy\n",
    "\n",
    "import resnet as rnet\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda_available else \"cpu\")\n",
    "print(f'''using device {device}''')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/r2/Documents/DD2424project\n"
     ]
    }
   ],
   "source": [
    "path = !pwd\n",
    "path = path[0]\n",
    "print(path)\n",
    "mdl_savefile = path+\"/models/resnet56_cumultrain_parameters.pickle\"\n",
    "mdl_loadfile = mdl_savefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model(model):\n",
    "    print(\"Model's state_dict:\")\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "def make_oneh(labels):\n",
    "    oneh_labels = np.zeros([labels.shape[0], 10])\n",
    "    indexes = np.array([np.arange(0,labels.shape[0]), labels.numpy()])\n",
    "    oneh_labels[tuple(indexes)] = 1.0\n",
    "    return torch.tensor(oneh_labels).long()\n",
    "\n",
    "def get_xent_acc(prediction, labels):\n",
    "    predicted_labels = np.argmax(prediction.cpu().data.numpy(),1)\n",
    "    (predicted_labels - labels.cpu().data.numpy())\n",
    "    return sum((predicted_labels - labels.cpu().data.numpy()) == 0)/predicted_labels.shape[0]\n",
    "\n",
    "def xent_softmax(prediction):\n",
    "    pred = np.exp(prediction.cpu().data.numpy())\n",
    "    divide = np.repeat(pred.sum(1).reshape(pred.shape[0],1), pred.shape[1],1)\n",
    "    pred = np.divide(pred,divide)\n",
    "    return pred \n",
    "\n",
    "def update_results(n_vbatches=1):\n",
    "    f\"updates loss and accuracy for training and validation\" \n",
    "    global accu_tr\n",
    "    global accu_val\n",
    "    global loss_tr\n",
    "    global loss_val\n",
    "    accu_tr  += [get_xent_acc(prediction, labels)]\n",
    "    loss_tr  += [output]\n",
    "    with torch.no_grad():\n",
    "        network.eval()      \n",
    "        accu, loss = 0, 0\n",
    "        for i, (imgs_val, labels_val) in enumerate(dataloader_vl):\n",
    "            imgs_val, labels_val = cuda(imgs_val), cuda(labels_val)\n",
    "            prediction_val = network.forward(imgs_val)  \n",
    "            accu += get_xent_acc(prediction_val, labels_val)\n",
    "            loss +=       lossfn(prediction_val, labels_val)\n",
    "            if i == n_vbatches:\n",
    "                break\n",
    "        loss_val = loss_val + [loss/(i+1)]\n",
    "        accu_val = accu_val + [accu/(i+1)]\n",
    "        network.train()\n",
    "\n",
    "def compare_parameters(net1, net2):\n",
    "    f\"\"\"compare parameters of two models with the same architecture\"\"\" \n",
    "    net1_param, net2_param, diff = [], [], 0\n",
    "    for param_tensor in net1.state_dict():\n",
    "        net1_param = net1_param + [net1.state_dict()[param_tensor]]\n",
    "        net2_param = net2_param + [net2.state_dict()[param_tensor]]\n",
    "    for i in range(len(net1_param)):\n",
    "        diff = diff + np.sum(net1_param[i].cpu().numpy() - net2_param[i].cpu().numpy())\n",
    "    print(f\"\"\"difference in weights and biases is: {diff} between network1 and network2\"\"\")\n",
    "\n",
    "def load_cifar10(bsize_tr,bsize_val=128):\n",
    "    torchvision.datasets.CIFAR10(path,download=True)\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])\n",
    "    cifar10_train = torchvision.datasets.CIFAR10(path, train=True, transform=transform)\n",
    "    cifar10_valid = torchvision.datasets.CIFAR10(path, train=False,transform=transform)\n",
    "    dataloader_tr = torch.utils.data.DataLoader(cifar10_train,\n",
    "                                              batch_size=bsize_tr,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=0,\n",
    "                                              pin_memory=True)\n",
    "    dataloader_val = torch.utils.data.DataLoader(cifar10_valid,\n",
    "                                              batch_size=bsize_val,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=0,\n",
    "                                              pin_memory=True)\n",
    "    return dataloader_tr, dataloader_val\n",
    "\n",
    "def cuda(thing):\n",
    "    if torch.cuda.is_available(): return thing.cuda()        \n",
    "    return thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning-rate range-test\n",
    "\n",
    "(1) a factor of 3 or 4 less than the maximum bound, (2) a factor of 10 or 20 less than the maximum bound if only one cycle is used, (3) by a short test of hundreds of iterations with a few initial learning rates and pick the largest one that allows convergence to begin without signs of overfitting as shown in Figure 1a (if the initial learning rate is too large, the training won’t begin to converge). Take note that there is a maximum speed the learning rate can increase without the training becoming unstable, which effects your choices for the minimum and maximum learning rates (i.e., increase the stepsize to increase the difference between the minimum and maximum).\n",
    "\n",
    "Learning rate (LR): Perform a learning rate range test to a “large” learning rate. The max LR depends on the architecture (for the shallow 3-layer architecture, large is 0.01 while for resnet, large is 3.0), you might try more than one maximum. Using the 1cycle LR policy with a maximum learning rate determined from an LR range test, a minimum learning rate as a tenth of the maximum appears to work well but other factors are relevant, such as the rate of learning rate increase (too fast and increase will cause instabilities).\n",
    "\n",
    "\n",
    "SuperConvergence paper recommends minimum learning-rate to be 1/3rd or 1/4th of the maximum learning-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lr      = 1e-9\n",
    "max_lr      = 1.5e1\n",
    "iterations  = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "time after 100 iterations: 9.288184404373169 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8404f47dcc03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprediction\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0moutput\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mlossfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mintvl_val\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mupdate_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_vbatches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_vbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# clear old gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# calculate new gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-0fd533723e80>\u001b[0m in \u001b[0;36mupdate_results\u001b[0;34m(n_vbatches)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mimgs_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mprediction_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0maccu\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mget_xent_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m       \u001b[0mlossfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_vbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-0fd533723e80>\u001b[0m in \u001b[0;36mget_xent_acc\u001b[0;34m(prediction, labels)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_xent_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lossfn      = nn.CrossEntropyLoss()\n",
    "dataloader_tr, dataloader_vl = load_cifar10(bsize_tr)\n",
    "start      = time.time()    \n",
    "n_vbatches = 10\n",
    "intvl_val  = 10\n",
    "intvl_time = 100 \n",
    "results_lrate = [] \n",
    "\n",
    "for bsize_tr in [128, 1024]:\n",
    "    accu_tr, accu_val, loss_tr, loss_val = [], [], [], []    \n",
    "    network   = rnet.resnet56()\n",
    "    if cuda_available: network.cuda()\n",
    "    optimizer = optim.SGD(network.parameters(), lr=min_lr)    \n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                                  min_lr, \n",
    "                                                  max_lr, \n",
    "                                                  step_size_up=iterations, \n",
    "                                                  step_size_down=0, \n",
    "                                                  mode='triangular', \n",
    "                                                  cycle_momentum=False, \n",
    "                                                  base_momentum=0)\n",
    "    i = 1\n",
    "    while i < iterations:\n",
    "        datait_tr = iter(dataloader_tr)\n",
    "        for n, (images, labels) in enumerate(datait_tr):\n",
    "            images, labels = cuda(images), cuda(labels)\n",
    "            prediction     = network.forward(images)\n",
    "            output         = lossfn(prediction, labels)\n",
    "            if i % intvl_val == 0: update_results(n_vbatches=n_vbatches)\n",
    "            optimizer.zero_grad()  # clear old gradients\n",
    "            output.backward()      # calculate new gradients \n",
    "            optimizer.step()       # updates weights\n",
    "            scheduler.step()       # change learning-rate in optimizer\n",
    "            if i % intvl_time == 0: print(f\"time after {i} iterations: {time.time() - start} seconds\")                \n",
    "            i += 1\n",
    "            if i >= iterations: break\n",
    "    f\"deepcopy results and make room for another run\"\n",
    "    results_lrate[j] = {'max_lr'  :max_lr,\n",
    "                        'bsize_tr':bsize_tr,\n",
    "                        'accu_tr' :np.array(copy.deepcopy(accu_tr)), \n",
    "                        'accu_val':np.array(copy.deepcopy(accu_val)), \n",
    "                        'loss_tr' :np.array(copy.deepcopy(loss_tr)), \n",
    "                        'loss_val':np.array(copy.deepcopy(loss_val))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment += 1\n",
    "x1, x2 = [np.linspace(min_lr, max_lr, num) for num in [len(accu_val), len(loss_val)]]\n",
    "plt.plot(x1, np.array(accu_val))\n",
    "save_file   = path+f\"/rangetest/ola/{experiment}LRRT_bs{bsize_tr}minlr{min_lr}maxlr{max_lr}iter{iterations}mom0wd0{arch}.png\"\n",
    "plt.savefig(save_file)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = [np.linspace(max_lr, min_lr, num) for num in [len(accuracy), len(loss)]]\n",
    "plt.plot(x1, np.array(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate best batchsize \n",
    "hyperparameters 1 recommends to use as large batchsize as possible, batchsize is choosen at the same time as choosing learningrate range\n",
    "\n",
    "Total batch size (TBS): A large batch size works well but the magnitude is typically con- strained by the GPU memory. If your server has multiple GPUs, the total batch size is the batch size on a GPU multiplied by the number of GPUs. If the architecture is small or your hardware permits very large batch sizes, then you might compare performance of different batch sizes. In addition, recall that small batch sizes add regularization while large batch sizes add less, so utilize this while balancing the proper amount of regularization. It is often better to use a larger batch size so a larger learning rate can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate best momentum value\n",
    "\n",
    "Momentum: Short runs with momentum values of 0.99, 0.97, 0.95, and 0.9 will quickly show the best value for momentum. If using the 1cycle learning rate schedule, it is better to use a cyclical momentum (CM) that starts at this maximum momentum value and decreases with increasing learning rate to a value of 0.8 or 0.85 (performance is almost independent of the minimum momentum value). Using cyclical momentum along with the LR range test stabilizes the convergence when using large learning rate values more than a constant momentum does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_max   = 1.0\n",
    "# lr_min   = lr_max/10.0\n",
    "# bsize_tr = 1024\n",
    "# step_it  = 300\n",
    "# mom_min  = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_tr, dataloader_vl = load_cifar10(bsize_tr) \n",
    "lossfn      = nn.CrossEntropyLoss()\n",
    "result_momentum = []\n",
    "intvl_val  = 10\n",
    "intvl_time = 10\n",
    "for j, mom_max in enumerate([0.9, 0.93, 0.95, 0.97, 0.99]):\n",
    "    accu_tr, accu_val, loss_tr, loss_val = [], [], [], []\n",
    "    network   = rnet.resnet56()\n",
    "    if cuda_available: network.cuda()\n",
    "    optimizer = optim.SGD(network.parameters(), lr=min_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              min_lr, \n",
    "                                              max_lr, \n",
    "                                              step_size_up=step_it, \n",
    "                                              step_size_down=step_it, \n",
    "                                              mode='triangular', \n",
    "                                              cycle_momentum=True, \n",
    "                                              base_momentum=mom_min,\n",
    "                                              max_momentum=mom_max)\n",
    "    f\"do short run\"\n",
    "    i = 1\n",
    "    while i < step_it*2:\n",
    "        datait_tr = iter(dataloader_tr)\n",
    "        for n, (images, labels) in enumerate(datait_tr):\n",
    "            images, labels = cuda(images), cuda(labels)\n",
    "            prediction     = network.forward(images)\n",
    "            output         = lossfn(prediction, labels)\n",
    "            if i % intvl_val  == 0: update_results(n_vbatches=10)                 \n",
    "            optimizer.zero_grad()  # clear old gradients\n",
    "            output.backward()      # calculate new gradients \n",
    "            optimizer.step()       # updates weights\n",
    "            scheduler.step()       # change learning-rate in optimizer\n",
    "            if i % intvl_time == 0: print(f\"time after {i} iterations: {time.time() - start} seconds\")                \n",
    "            i += 1                    \n",
    "            if i >= step_it*2   : break\n",
    "    momentum_test(mom_max,mom_min=0.8)\n",
    "    \n",
    "    f\"deepcopy results and make room for another run\"\n",
    "    results_momentum[j] = {'mom_max' :mom_max,\n",
    "                           'accu_tr' :np.array(copy.deepcopy(accu_tr)), \n",
    "                           'accu_val':np.array(copy.deepcopy(accu_val)), \n",
    "                           'loss_tr' :np.array(copy.deepcopy(loss_tr)), \n",
    "                           'loss_val':np.array(copy.deepcopy(loss_val))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"analyse momentum results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate best weight-decay\n",
    "\n",
    "Weight decay (WD): This requires a grid search to determine the proper magnitude but usually does not require more than one significant figure accuracy. Use your knowl- edge of the dataset and architecture to decide which values to test. For example, a more complex dataset requires less regularization so test smaller weight decay values, such as 10−4, 10−5, 10−6, 0. A shallow architecture requires more regularization so test larger weight decay values, such as 10−2, 10−3, 10−4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_max   = 1.0\n",
    "# lr_min   = lr_max/10.0\n",
    "# bsize_tr = 1024\n",
    "# mom_max  = \n",
    "# mom_min  = 0.8\n",
    "# step_epo = 5\n",
    "# step_it  = 50000/bsize_tr*step_epo\n",
    "# anne_epo = 2 \n",
    "# anne_it  = 50000/bsize_tr*anne_epo\n",
    "# cycles   = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in hyperparameter1 the number of steps up and down are described in epochs – which depends on the batchsize\n",
    "cifar10 has 50.000 images. each epoch has 50000/bsize_tr steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_tr, dataloader_vl = load_cifar10(bsize_tr)\n",
    "lossfn      = nn.CrossEntropyLoss()\n",
    "results_weightdecay = [] \n",
    "intvl_val  = 10\n",
    "intvl_time = 10\n",
    "for j, wd in enumerate([1e-4, 1e-5, 1e-6]):\n",
    "    accu_tr, accu_val, loss_tr, loss_val = [], [], [], []\n",
    "    network   = rnet.resnet56()\n",
    "    if cuda_available: network.cuda()\n",
    "    optimizer = optim.SGD(network.parameters(), lr=min_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                                  min_lr, \n",
    "                                                  max_lr, \n",
    "                                                  step_size_up=step_it, \n",
    "                                                  step_size_down=step_it, \n",
    "                                                  mode='triangular', \n",
    "                                                  cycle_momentum=True, \n",
    "                                                  base_momentum=mom_min,\n",
    "                                                  max_momentum=mom_max)\n",
    "    f\"do Cyclic Learning-Rate training\"\n",
    "    for cycle in range(1,cycles):\n",
    "        i = 1\n",
    "        while i < step_it*2:\n",
    "            datait_tr = iter(dataloader_tr)\n",
    "            for n, (images, labels) in enumerate(datait_tr):\n",
    "                images, labels = cuda(images), cuda(labels)\n",
    "                prediction     = network.forward(images)\n",
    "                output         = lossfn(prediction, labels)\n",
    "                if i % intvl_val  == 0: update_results(n_vbatches=10)                 \n",
    "                optimizer.zero_grad()  # clear old gradients\n",
    "                output.backward()      # calculate new gradients \n",
    "                optimizer.step()       # updates weights\n",
    "                scheduler.step()       # change learning-rate in optimizer\n",
    "                if i % intvl_time == 0: print(f\"time after {i} iterations: {time.time() - start} seconds\")                \n",
    "                i += 1                    \n",
    "                if i >= step_it*2   : break\n",
    "    f\"do annealing\"\n",
    "    \n",
    "    f\"deepcopy results and make room for another run\"\n",
    "    results_weightdecay[j] = {'wd'      :wd,\n",
    "                              'accu_tr' :np.array(copy.deepcopy(accu_tr)), \n",
    "                              'accu_val':np.array(copy.deepcopy(accu_val)), \n",
    "                              'loss_tr' :np.array(copy.deepcopy(loss_tr)), \n",
    "                              'loss_val':np.array(copy.deepcopy(loss_val))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"analyse weightdecay results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now do proper training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 128\n",
    "dataloader_tr, dataloader_vl = load_cifar10(batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "f\"\"\"To get ResNet56 use ResNet(BasicBlock,[9,9,9])\"\"\" \n",
    "resnet56 = ResNet(BasicBlock,[9,9,9])\n",
    "if cuda_available: resnet56.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss-function    https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_xent = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimize-function https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(resnet56.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# play with forward and backwards pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start    = time.time()\n",
    "f\"\"\"load previous model\"\"\"\n",
    "# if os.path.isfile(load_filename):\n",
    "#     resnet.load_state_dict(torch.load(load_filename))\n",
    "n_epoch  = 1\n",
    "validation_interval = 10\n",
    "\n",
    "trainacc  = [] \n",
    "valacc    = [] \n",
    "trainloss = []\n",
    "valloss   = [] \n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    train_iterator = iter(train_data_loader)\n",
    "    for i in range(len(train_iterator)):\n",
    "        images, labels = train_iterator.next()\n",
    "        \n",
    "        f\"\"\"if we use GPU, input- and target-tensors must be loaded on GPU as well.\"\"\"\n",
    "        if cuda_available: images, labels = images.cuda(), labels.cuda()\n",
    "            \n",
    "        prediction = resnet.forward(images)\n",
    "        output     = loss_xent(prediction, labels)\n",
    "        \n",
    "        trainacc = trainacc + [get_xent_acc(prediction, labels)]\n",
    "        trainloss = trainloss + [output]\n",
    "        \n",
    "        f\"\"\"using zero_grad() seems to be necessary. step() does not clear the gradients\"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        output.backward() # calculates gradients \n",
    "        optimizer.step()  # updates weights\n",
    "        \n",
    "        if i % validation_interval == 0:\n",
    "            valacc, valloss = get_valacc(valacc, valloss)\n",
    "            \n",
    "    print(f\"\"\"time passed after training {epoch+1} epochs is {time.time()-start} seconds\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, x3 = [np.linspace(0, len(trainacc), num) for num in [len(trainloss), len(trainacc), len(valacc)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x1,np.array(trainloss))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x2,np.array(trainacc))\n",
    "plt.plot(x3,np.array(valacc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load Model\n",
    "### https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "f\"\"\"save\"\"\"\n",
    "torch.save(resnet.state_dict(), save_filename)\n",
    "f\"\"\"load\"\"\"\n",
    "resnet2 = ResNet(BasicBlock,[9,9,9])\n",
    "resnet2.load_state_dict(torch.load(load_filename))\n",
    "compare_parameters(resnet,resnet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(torchvision.utils.make_grid(images.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet = models.densenet161()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
